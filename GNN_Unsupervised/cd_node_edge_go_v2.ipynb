{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyod.models.ecod import ECOD\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from utils.utils_go import *\n",
    "from dgi.utils_dgi import *\n",
    "from vgae.utils_vgae import *\n",
    "from vgae.utils_vgae_tg import *\n",
    "from unsupervised_models.models import *\n",
    "\n",
    "# os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "# %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.version.cuda)\"\n",
    "!python -c \"import torch; print(torch.__version__)\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp:\t\t exp25\n",
      "Methods:\t ['dgi']\n",
      "Data variations: ['none']\n",
      "Dimension:\t 3\n",
      "Threshold corr:\t 0.01\n",
      "Iterations:\t 1\n",
      "Groups id:\t ['pck1', 'zwf1', 'WT']\n",
      "Subgroups id:\t {'pck1': ['1', '2', '3'], 'zwf1': ['1', '2'], 'WT': ['1', '2', '3', '4', '5']}\n",
      "Seeds:\t\t [42, 43, 44, 45, 46]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"exp.json\")\n",
    "experiment = json.load(file)\n",
    "exp_num = experiment[\"exp\"]\n",
    "\n",
    "file = open(\"output/{}/parameters.json\".format(exp_num))\n",
    "params = json.load(file)\n",
    "\n",
    "exp = params[\"exp\"]\n",
    "print(\"Exp:\\t\\t\", exp)\n",
    "\n",
    "methods = params[\"methods\"]\n",
    "print(\"Methods:\\t\", methods)\n",
    "\n",
    "data_variations = params[\"data_variations\"]\n",
    "print(\"Data variations:\", data_variations)\n",
    "\n",
    "dimension = params[\"dimension\"]\n",
    "print(\"Dimension:\\t\", dimension)\n",
    "\n",
    "threshold_corr = params[\"threshold_corr\"]\n",
    "print(\"Threshold corr:\\t\", threshold_corr)\n",
    "\n",
    "iterations = params[\"iterations\"]\n",
    "print(\"Iterations:\\t\", iterations)\n",
    "\n",
    "groups_id = params[\"groups_id\"]\n",
    "print(\"Groups id:\\t\", groups_id)\n",
    "\n",
    "subgroups_id_ = params[\"subgroups_id\"]\n",
    "print(\"Subgroups id:\\t\", subgroups_id_)\n",
    "\n",
    "seeds = params[\"seeds\"]\n",
    "print(\"Seeds:\\t\\t\", seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node-Edge embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroups id:\t {'pck1': ['1', '2', '3'], 'zwf1': ['1', '2'], 'WT': ['1', '2', '3', '4', '5']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:60: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Caught ImportError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/loader/node_loader.py\", line 134, in collate_fn\n    out = self.node_sampler.sample_from_nodes(input_data)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py\", line 246, in sample_from_nodes\n    out = node_sample(inputs, self._sample)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py\", line 504, in node_sample\n    out = sample_fn(seed, seed_time)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py\", line 421, in _sample\n    raise ImportError(f\"'{self.__class__.__name__}' requires \"\nImportError: 'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m dataset \u001b[39m=\u001b[39m CustomDataset(nodes_data, edges_data, transform\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m model \u001b[39m=\u001b[39m DGI_Inductive(dataset, dimension, device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(epochs\u001b[39m=\u001b[39;49m\u001b[39m300\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m model\u001b[39m.\u001b[39msave_node_embeddings(\u001b[39m\"\u001b[39m\u001b[39moutput/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/node_embeddings/node-embeddings_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(exp, method, group, subgroup, iteration))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" transform = T.Compose([\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m    # T.NormalizeFeatures(),\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m    T.ToDevice(device),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39mmodel.fit(epochs=300)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.19.4.230/home/ealvarez/Project/GNN_Unsupervised/cd_node_edge_go.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mmodel.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration)) \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/GNN_Unsupervised/unsupervised_models/models.py:230\u001b[0m, in \u001b[0;36mDGI_Inductive.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, epochs):\n\u001b[1;32m    229\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m--> 230\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    231\u001b[0m         \u001b[39mif\u001b[39;00m epoch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    232\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m>2\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Project/GNN_Unsupervised/unsupervised_models/models.py:204\u001b[0m, in \u001b[0;36mDGI_Inductive.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    203\u001b[0m total_loss \u001b[39m=\u001b[39m total_examples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 204\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loader:\n\u001b[1;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    206\u001b[0m     pos_z, neg_z, summary \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(batch\u001b[39m.\u001b[39mx, batch\u001b[39m.\u001b[39medge_index,\n\u001b[1;32m    207\u001b[0m                                 batch\u001b[39m.\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/loader/base.py:36\u001b[0m, in \u001b[0;36mDataLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_fn(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator))\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mImportError\u001b[0m: Caught ImportError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/loader/node_loader.py\", line 134, in collate_fn\n    out = self.node_sampler.sample_from_nodes(input_data)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py\", line 246, in sample_from_nodes\n    out = node_sample(inputs, self._sample)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py\", line 504, in node_sample\n    out = sample_fn(seed, seed_time)\n  File \"/home/ealvarez/miniconda3/envs/meta_net_3.10/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py\", line 421, in _sample\n    raise ImportError(f\"'{self.__class__.__name__}' requires \"\nImportError: 'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'\n"
     ]
    }
   ],
   "source": [
    "# read raw data\n",
    "df_join_raw = pd.read_csv(\"input/{}_raw.csv\".format(exp), index_col=0)\n",
    "df_join_raw = df_join_raw.iloc[:, 2:]\n",
    "df_join_raw\n",
    "\n",
    "# log10\n",
    "df_join_raw_log = log10_global(df_join_raw)\n",
    "df_join_raw_log.head()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# node-embeddings + edge-embeddings\n",
    "for method in methods: # change\n",
    "    for data_variation in data_variations: # change   \n",
    "        for iteration in range(iterations):\n",
    "            # ---\n",
    "            # Node embeddings\n",
    "            # ---\n",
    "            subgroups_id = subgroups_id_.copy()\n",
    "            \n",
    "            torch.manual_seed(seeds[iteration])\n",
    "            np.random.seed(seeds[iteration])\n",
    "            \n",
    "            if data_variation != \"none\":\n",
    "                for group in groups_id:\n",
    "                    subgroups_id[group] = [data_variation]\n",
    "            print(\"Subgroups id:\\t\", subgroups_id)\n",
    "            \n",
    "            for group in tqdm(groups_id):\n",
    "                for subgroup in tqdm(subgroups_id[group]):\n",
    "                    nodes_data = pd.read_csv(\"output/{}/preprocessing/graphs_data/nodes_data_{}_{}.csv\".format(exp, group, subgroup)).iloc[:, 2:]\n",
    "                    edges_data = pd.read_csv(\"output/{}/preprocessing/graphs_data/edges_data_{}_{}.csv\".format(exp, group, subgroup))\n",
    "\n",
    "                    if method == \"dgi\":\n",
    "                        \"\"\" data = CustomDatasetDGI(\"g_{}_{}\".format(group, subgroup), nodes_data, edges_data)\n",
    "                        graph = data[0]\n",
    "                        \n",
    "                        # train\n",
    "                        args_ = args_dgi(dimension)\n",
    "                        train_dgi(exp, graph, args_, method, group, subgroup, iteration) \"\"\"\n",
    "                        \n",
    "                        \"\"\" transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(),\n",
    "                            T.ToDevice(device),\n",
    "                            # T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
    "                            # T.RandomNodeSplit(num_val=0.05, num_test=0.1),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        model = DGI_Transductive(dataset, dimension, device)\n",
    "                        model.fit(epochs=300)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration)) \"\"\"\n",
    "                        \n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=None)\n",
    "                        model = DGI_Inductive(dataset, dimension, device)\n",
    "                        model.fit(epochs=300)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration))\n",
    "                        \n",
    "                        \"\"\" transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(),\n",
    "                            T.ToDevice(device),\n",
    "                            T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                                            split_labels=True, add_negative_train_samples=False),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        model = ARGVA_Base(dataset, dimension, device)\n",
    "                        model.fit(epochs=300)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration)) \"\"\"\n",
    "                        \n",
    "                        \"\"\" transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(),\n",
    "                            T.ToDevice(device),\n",
    "                            T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
    "                            # T.RandomNodeSplit(num_val=0.05, num_test=0.1),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        model = VGAE_Linear(dataset, dimension, device)\n",
    "                        model.fit(epochs=300)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration)) \"\"\"\n",
    "                        \n",
    "                    elif \"vgae\":\n",
    "                        \"\"\" data = CustomDatasetVGAE(\"g_{}_{}\".format(group, subgroup), nodes_data, edges_data)\n",
    "                        graph = data[0]\n",
    "\n",
    "                        # train\n",
    "                        args_ = args_vgae(dimension)\n",
    "                        train_vgae(exp, graph, args_, method, group, subgroup, iteration) \"\"\"\n",
    "                        \n",
    "                        \"\"\" transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(),\n",
    "                            T.ToDevice(device),\n",
    "                            T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
    "                            # T.RandomNodeSplit(num_val=0.05, num_test=0.1),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        train_data, val_data, test_data = dataset[0]\n",
    "                        dataset.info()\n",
    "\n",
    "                        # train\n",
    "                        model = VGAE(Encoder(dataset.num_features, dimension)).to(device)\n",
    "                        train_vgae_tg(exp, model, train_data, test_data, method, group, subgroup, iteration) \"\"\"\n",
    "                        \n",
    "                        \"\"\" transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(),\n",
    "                            T.ToDevice(device),\n",
    "                            T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
    "                            # T.RandomNodeSplit(num_val=0.05, num_test=0.1),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        model = VGAE_Base(dataset, dimension, device)\n",
    "                        model.fit(epochs=300)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration)) \"\"\"\n",
    "                  \n",
    "            # ---\n",
    "            # Edge embeddings\n",
    "            # ---\n",
    "            subgroups_id = subgroups_id_.copy()\n",
    "            print(method, data_variation)\n",
    "            \n",
    "            if data_variation != \"none\":\n",
    "                subgroups_id_op = {}\n",
    "                for group in groups_id:\n",
    "                    subgroups_id_op[group] = [data_variation]\n",
    "            else:\n",
    "                subgroups_id_op = subgroups_id\n",
    "            print(\"Subgroups id op:\", subgroups_id_op)\n",
    "            \n",
    "            edge_embeddings_global(exp, method, groups_id, subgroups_id_op, iteration)\n",
    "            \n",
    "            for group in tqdm(groups_id):\n",
    "                df_edge_embeddings_concat = pd.DataFrame()\n",
    "                k = 0\n",
    "                for subgroup in tqdm(subgroups_id_op[group]):\n",
    "                    df_edge_embeddings = pd.read_csv(\"output/{}/edge_embeddings/edge-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration))\n",
    "                    df_edge_embeddings[\"subgroup\"] = [k] * len(df_edge_embeddings)\n",
    "                    df_edge_embeddings_concat = pd.concat([df_edge_embeddings_concat, df_edge_embeddings])\n",
    "                    k += 1\n",
    "                \n",
    "                df_edge_embeddings_concat.to_csv(\"output/{}/edge_embeddings/edge-embeddings_concat_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration), index=False)\n",
    "                    \n",
    "            # outlier detection (ECOD)\n",
    "            # dict_df_edge_embeddings_concat_outlier = {}\n",
    "            dict_df_edge_embeddings_concat_filter = {}\n",
    "\n",
    "            for group in tqdm(groups_id):\n",
    "                df_edge_embeddings_concat = pd.read_csv(\"output/{}/edge_embeddings/edge-embeddings_concat_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration))\n",
    "\n",
    "                X_train = df_edge_embeddings_concat.iloc[:, 2:-1]\n",
    "\n",
    "                clf = ECOD()\n",
    "                clf.fit(X_train)\n",
    "\n",
    "                X_train[\"labels\"] = clf.labels_ # binary labels (0: inliers, 1: outliers)\n",
    "\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat.copy()\n",
    "                df_edge_embeddings_concat_filter[\"labels\"] = clf.labels_\n",
    "\n",
    "                # save\n",
    "                df_edge_embeddings_concat_filter.to_csv(\"output/{}/edge_embeddings/edge-embeddings_concat_outlier_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration), index=False)\n",
    "                \n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter[df_edge_embeddings_concat_filter[\"labels\"] == 0].copy()\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter.iloc[:, :-1]\n",
    "\n",
    "                # dict_df_edge_embeddings_concat_outlier[group] = X_train\n",
    "                dict_df_edge_embeddings_concat_filter[group] = df_edge_embeddings_concat_filter\n",
    "                \n",
    "            # mapping idx with id\n",
    "            for group in tqdm(groups_id):\n",
    "                df_aux = pd.DataFrame(())\n",
    "                k = 0\n",
    "                for subgroup in subgroups_id_op[group]:\n",
    "                    df_nodes = pd.read_csv(\"output/{}/preprocessing/graphs_data/nodes_data_{}_{}.csv\".format(exp, group, subgroup))\n",
    "                    dict_id = dict(zip(df_nodes[\"idx\"], df_nodes[\"id\"]))\n",
    "\n",
    "                    # mapping\n",
    "                    df_edge_embeddings_concat_filter = dict_df_edge_embeddings_concat_filter[group]\n",
    "                    df_edge_embeddings_concat_filter_aux = df_edge_embeddings_concat_filter[df_edge_embeddings_concat_filter[\"subgroup\"] == k].copy()\n",
    "                    \n",
    "                    # print(df_edge_embeddings_concat_filter)\n",
    "                    df_edge_embeddings_concat_filter_aux[\"source\"] = df_edge_embeddings_concat_filter_aux[\"source\"].map(dict_id)\n",
    "                    df_edge_embeddings_concat_filter_aux[\"target\"] = df_edge_embeddings_concat_filter_aux[\"target\"].map(dict_id)\n",
    "                    df_aux = pd.concat([df_aux, df_edge_embeddings_concat_filter_aux])\n",
    "                    k += 1\n",
    "                dict_df_edge_embeddings_concat_filter[group] = df_aux\n",
    "                \n",
    "            # format id\n",
    "            if data_variation != \"none\":\n",
    "                for group in tqdm(groups_id):\n",
    "                    # format\n",
    "                    df_edge_embeddings_concat_filter = dict_df_edge_embeddings_concat_filter[group]\n",
    "                    df_edge_embeddings_concat_filter[\"source\"] = df_edge_embeddings_concat_filter[\"source\"].map(lambda x: int(x[1:]))\n",
    "                    df_edge_embeddings_concat_filter[\"target\"] = df_edge_embeddings_concat_filter[\"target\"].map(lambda x: int(x[1:]))\n",
    "                        \n",
    "            # filter by different edges\n",
    "            if data_variation != \"none\":\n",
    "                for group in tqdm(groups_id):\n",
    "                    df_edge_embeddings_concat_filter = dict_df_edge_embeddings_concat_filter[group]\n",
    "                    df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter[df_edge_embeddings_concat_filter[\"source\"] != df_edge_embeddings_concat_filter[\"target\"]].copy()\n",
    "                    dict_df_edge_embeddings_concat_filter[group] = df_edge_embeddings_concat_filter\n",
    "                    \n",
    "            # count edges and filter by count\n",
    "            dict_df_edges_filter = {}\n",
    "            for group in tqdm(groups_id):\n",
    "                # read\n",
    "                df_edge_embeddings_concat_filter = dict_df_edge_embeddings_concat_filter[group]\n",
    "                \n",
    "                # sort edges\n",
    "                sort_df_edges(df_edge_embeddings_concat_filter)\n",
    "\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter[[\"source\", \"target\"]].value_counts().to_frame()\n",
    "                df_edge_embeddings_concat_filter.reset_index(inplace=True)\n",
    "                df_edge_embeddings_concat_filter.columns = [\"source\", \"target\", \"count\"]\n",
    "                \n",
    "                # filter\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter[df_edge_embeddings_concat_filter[\"count\"] == len(subgroups_id[group])]\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter.iloc[:, [0, 1]]\n",
    "                dict_df_edges_filter[group] = df_edge_embeddings_concat_filter\n",
    "                \n",
    "                df_edge_embeddings_concat_filter.sort_values([\"source\", \"target\"], ascending=True, inplace=True)\n",
    "                df_edge_embeddings_concat_filter.to_csv(\"output/{}/common_edges/common_edges_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union\n",
      "dgi pck1 none\n",
      "Union\n",
      "dgi zwf1 none\n",
      "Union\n",
      "dgi WT none\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "list_details = []\n",
    "\n",
    "for method in methods:  \n",
    "    for k, group in enumerate(groups_id):        \n",
    "        dict_df_edges_filter = {}\n",
    "        dict_df_corr = {}\n",
    "        dict_df_edges_filter_weight = {}\n",
    "    \n",
    "        for data_variation in data_variations:\n",
    "            list_common_subgraph = []\n",
    "            for iteration in range(iterations):\n",
    "                df_edges_filter_weight_filter = pd.read_csv(\"output/{}/common_edges/common_edges_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration))\n",
    "                # print(df_edges_filter_weight_filter)\n",
    "\n",
    "                G = nx.from_pandas_edgelist(df_edges_filter_weight_filter) # last change: create_using=nx.Graph() # , edge_attr=[\"weight\"])\n",
    "                # SG = G.subgraph([0, 1, 2, 3, 4, 5])\n",
    "                # graph_partial_detail(SG, edges=True)\n",
    "                list_common_subgraph.append(G)\n",
    "                \n",
    "            print(\"Union\")\n",
    "            # union\n",
    "            U = nx.compose_all(list_common_subgraph)\n",
    "            \n",
    "            df_compose_subgraph = nx.to_pandas_edgelist(U)\n",
    "            dict_df_edges_filter[group] = df_compose_subgraph.iloc[:, [0, 1]]\n",
    "            \n",
    "            # correlation\n",
    "            nodes = list(U.nodes())\n",
    "            \n",
    "            df_join_raw_filter = df_join_raw_log.loc[nodes, :]\n",
    "            # df_join_raw_filter = df_join_raw_filter.filter(regex=group, axis=1)\n",
    "            df_join_raw_filter = df_join_raw_filter.filter(like=group, axis=1)\n",
    "\n",
    "            df_join_raw_filter_t= df_join_raw_filter.T\n",
    "            # df_join_raw_filter_corr = df_join_raw_filter_t.corr(method=\"pearson\")\n",
    "            df_join_raw_filter_corr = pg.pcorr(df_join_raw_filter_t)\n",
    "            dict_df_corr[group] = df_join_raw_filter_corr\n",
    "            \n",
    "            # get new correlation\n",
    "            df_edges_filter_weight = dict_df_edges_filter[group].copy()\n",
    "            df_corr = dict_df_corr[group]\n",
    "\n",
    "            df_edges_filter_weight[\"weight\"] = df_edges_filter_weight.apply(lambda x: df_corr.loc[x[\"source\"], x[\"target\"]], axis=1)\n",
    "            df_edges_filter_weight.sort_values([\"source\", \"target\"], ascending=True, inplace=True)\n",
    "            dict_df_edges_filter_weight[group] = df_edges_filter_weight\n",
    "            \n",
    "            # common subgraph\n",
    "            df_edges_filter_weight = dict_df_edges_filter_weight[group]\n",
    "            # G = nx.from_pandas_edgelist(df_edges_filter_weight, \"source\", \"target\", edge_attr=\"weight\")\n",
    "            print(method, group, data_variation)\n",
    "            # print(\"Before\")\n",
    "            # graph_partial_detail(G, edges=True)\n",
    "                \n",
    "            # filter by abs(weight) >= threshold\n",
    "            df_edges_filter_weight = dict_df_edges_filter_weight[group]\n",
    "            df_edges_filter_weight_filter = df_edges_filter_weight[df_edges_filter_weight[\"weight\"].abs() >= threshold_corr]\n",
    "            df_edges_filter_weight_filter.to_csv(\"output/{}/common_edges/common_edges_{}_{}_{}.csv\".format(exp, method, group, data_variation), index=False)\n",
    "            \n",
    "            # print(\"After\")\n",
    "            # graph_partial_detail(G, edges=True)\n",
    "            G = nx.from_pandas_edgelist(df_edges_filter_weight_filter, \"source\", \"target\", edge_attr=\"weight\")\n",
    "            list_details.append([method, group, data_variation, G.number_of_nodes(), G.number_of_edges(), nx.density(G)])\n",
    "\n",
    "df_details = pd.DataFrame(list_details, columns=[\"Method\", \"Group\", \"Data var.\", \"Num. nodes\", \"Num. edges\", \"Density\"])\n",
    "df_details.to_csv(\"output/{}/common_edges/summary.csv\".format(exp), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
