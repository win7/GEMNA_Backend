{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from pyod.models.ecod import ECOD\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from utils.utils_go import *\n",
    "# from dgi.utils_dgi import *\n",
    "# from vgae.utils_vgae import *\n",
    "# from vgae.utils_vgae_tg import *\n",
    "from unsupervised_models.models import *\n",
    "\n",
    "# os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "# %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "!python -c \"import torch; print(torch.version.cuda)\"\n",
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp:\t\t exp7\n",
      "Methods:\t ['vgae-base']\n",
      "Data variations: ['str']\n",
      "Dimension:\t 3\n",
      "Threshold corr:\t 0.1\n",
      "Iterations:\t 2\n",
      "Groups id:\t ['zwf1', 'pck1', 'WT']\n",
      "Subgroups id:\t {'zwf1': ['1', '2', '3'], 'pck1': ['1', '2'], 'WT': ['1', '2', '3', '4', '5']}\n",
      "Seeds:\t\t [42, 43, 44, 45, 46]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"exp.json\")\n",
    "experiment = json.load(file)\n",
    "exp_num = experiment[\"exp\"]\n",
    "\n",
    "file = open(\"output/{}/parameters.json\".format(exp_num))\n",
    "params = json.load(file)\n",
    "\n",
    "exp = params[\"exp\"]\n",
    "print(\"Exp:\\t\\t\", exp)\n",
    "\n",
    "methods = params[\"methods\"]\n",
    "print(\"Methods:\\t\", methods)\n",
    "\n",
    "data_variations = params[\"data_variations\"]\n",
    "print(\"Data variations:\", data_variations)\n",
    "\n",
    "dimension = params[\"dimension\"]\n",
    "print(\"Dimension:\\t\", dimension)\n",
    "\n",
    "threshold_corr = params[\"threshold_corr\"]\n",
    "print(\"Threshold corr:\\t\", threshold_corr)\n",
    "\n",
    "iterations = params[\"iterations\"]\n",
    "print(\"Iterations:\\t\", iterations)\n",
    "\n",
    "groups_id = params[\"groups_id\"]\n",
    "print(\"Groups id:\\t\", groups_id)\n",
    "\n",
    "subgroups_id_ = params[\"subgroups_id\"]\n",
    "print(\"Subgroups id:\\t\", subgroups_id_)\n",
    "\n",
    "seeds = params[\"seeds\"]\n",
    "print(\"Seeds:\\t\\t\", seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node-Edge embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ealvarez/miniconda3/envs/metanet_3.10/lib/python3.10/site-packages/pandas/core/arraylike.py:396: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n",
      "Subgroups id:\t {'zwf1': ['str'], 'pck1': ['str'], 'WT': ['str']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 50: 100%|██████████| 50/50 [15:59<00:00, 19.18s/it, Loss: 1.5892]\n",
      "Epoch: 50: 100%|██████████| 50/50 [20:49<00:00, 25.00s/it, Loss: 1.7328] \n",
      "Epoch: 18:  34%|███▍      | 17/50 [11:08<21:45, 39.57s/it, Loss: 1.7335]"
     ]
    }
   ],
   "source": [
    "# read raw data\n",
    "df_join_raw = pd.read_csv(\"input/{}_raw.csv\".format(exp), index_col=0)\n",
    "df_join_raw = df_join_raw.iloc[:, 2:]\n",
    "df_join_raw\n",
    "\n",
    "# log10\n",
    "df_join_raw_log = log10_global(df_join_raw)\n",
    "df_join_raw_log.head()\n",
    "\n",
    "epochs = 5 # change\n",
    "cuda = 1    # change\n",
    "device = torch.device('cuda:{}'.format(cuda) if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# node-embeddings + edge-embeddings\n",
    "for method in methods: # change\n",
    "    for data_variation in data_variations: # change   \n",
    "        for iteration in range(iterations):\n",
    "            # ---\n",
    "            # Node embeddings\n",
    "            # ---\n",
    "            subgroups_id = subgroups_id_.copy()\n",
    "            \n",
    "            torch.manual_seed(seeds[iteration])\n",
    "            np.random.seed(seeds[iteration])\n",
    "            \n",
    "            if data_variation != \"none\":\n",
    "                for group in groups_id:\n",
    "                    subgroups_id[group] = [data_variation]\n",
    "            print(\"Subgroups id:\\t\", subgroups_id)\n",
    "            \n",
    "            for group in groups_id:\n",
    "                for subgroup in subgroups_id[group]:\n",
    "                    nodes_data = pd.read_csv(\"output/{}/preprocessing/graphs_data/nodes_data_{}_{}.csv\".format(exp, group, subgroup)).iloc[:, 2:]\n",
    "                    edges_data = pd.read_csv(\"output/{}/preprocessing/graphs_data/edges_data_{}_{}.csv\".format(exp, group, subgroup))\n",
    "\n",
    "                    if method == \"dgi\":\n",
    "                        data = CustomDatasetDGI(\"g_{}_{}\".format(group, subgroup), nodes_data, edges_data)\n",
    "                        graph = data[0]\n",
    "                        \n",
    "                        args_ = args_dgi(dimension)\n",
    "                        train_dgi(exp, graph, args_, method, group, subgroup, iteration)\n",
    "                    \n",
    "                    elif method == \"dgi-tran\":\n",
    "                        transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(), #\n",
    "                            T.ToDevice(device),\n",
    "                            # T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        model = DGI_Transductive(dataset, dimension, device)\n",
    "                        model.fit(epochs=epochs)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration))\n",
    "                    \n",
    "                    elif method == \"dgi-indu\":\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=None)\n",
    "                        model = DGI_Inductive(dataset, dimension, device)\n",
    "                        model.fit(epochs=epochs)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration))\n",
    "                        \n",
    "                    elif method == \"vgae\":\n",
    "                        data = CustomDatasetVGAE(\"g_{}_{}\".format(group, subgroup), nodes_data, edges_data)\n",
    "                        graph = data[0]\n",
    "\n",
    "                        # train\n",
    "                        args_ = args_vgae(dimension)\n",
    "                        train_vgae(exp, graph, args_, method, group, subgroup, iteration)\n",
    "                        \n",
    "                    elif method == \"vgae-base\":\n",
    "                        transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(), #\n",
    "                            T.ToDevice(device),\n",
    "                            T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        model = VGAE_Base(dataset, dimension, device)\n",
    "                        model.fit(epochs=epochs)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration))\n",
    "\n",
    "                    elif method == \"vgae-line\":\n",
    "                        transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(), #\n",
    "                            T.ToDevice(device),\n",
    "                            T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True, split_labels=True, add_negative_train_samples=False),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        model = VGAE_Linear(dataset, dimension, device)\n",
    "                        model.fit(epochs=epochs)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration))\n",
    "                    \n",
    "                    elif method == \"argva-base\":\n",
    "                        transform = T.Compose([\n",
    "                            # T.NormalizeFeatures(), #\n",
    "                            T.ToDevice(device),\n",
    "                            T.RandomLinkSplit(num_val=0.05, num_test=0.1, is_undirected=True,\n",
    "                                            split_labels=True, add_negative_train_samples=False),\n",
    "                        ])\n",
    "                        dataset = CustomDataset(nodes_data, edges_data, transform=transform)\n",
    "                        model = ARGVA_Base(dataset, dimension, device)\n",
    "                        model.fit(epochs=epochs)\n",
    "                        model.save_node_embeddings(\"output/{}/node_embeddings/node-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration))\n",
    "                \n",
    "            # ---\n",
    "            # Edge embeddings\n",
    "            # ---\n",
    "            subgroups_id = subgroups_id_.copy()\n",
    "            print(method, data_variation)\n",
    "            \n",
    "            if data_variation != \"none\":\n",
    "                subgroups_id_op = {}\n",
    "                for group in groups_id:\n",
    "                    subgroups_id_op[group] = [data_variation]\n",
    "            else:\n",
    "                subgroups_id_op = subgroups_id\n",
    "            print(\"Subgroups id op:\", subgroups_id_op)\n",
    "            \n",
    "            edge_embeddings_global(exp, method, groups_id, subgroups_id_op, iteration)\n",
    "            \n",
    "            for group in tqdm(groups_id):\n",
    "                df_edge_embeddings_concat = pd.DataFrame()\n",
    "                k = 0\n",
    "                for subgroup in tqdm(subgroups_id_op[group]):\n",
    "                    df_edge_embeddings = pd.read_csv(\"output/{}/edge_embeddings/edge-embeddings_{}_{}_{}_{}.csv\".format(exp, method, group, subgroup, iteration))\n",
    "                    df_edge_embeddings[\"subgroup\"] = [k] * len(df_edge_embeddings)\n",
    "                    df_edge_embeddings_concat = pd.concat([df_edge_embeddings_concat, df_edge_embeddings])\n",
    "                    k += 1\n",
    "                \n",
    "                df_edge_embeddings_concat.to_csv(\"output/{}/edge_embeddings/edge-embeddings_concat_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration), index=False)\n",
    "                    \n",
    "            # outlier detection (ECOD)\n",
    "            # dict_df_edge_embeddings_concat_outlier = {}\n",
    "            dict_df_edge_embeddings_concat_filter = {}\n",
    "\n",
    "            for group in tqdm(groups_id):\n",
    "                df_edge_embeddings_concat = pd.read_csv(\"output/{}/edge_embeddings/edge-embeddings_concat_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration))\n",
    "\n",
    "                X_train = df_edge_embeddings_concat.iloc[:, 2:-1]\n",
    "\n",
    "                clf = ECOD()\n",
    "                clf.fit(X_train)\n",
    "\n",
    "                X_train[\"labels\"] = clf.labels_ # binary labels (0: inliers, 1: outliers)\n",
    "\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat.copy()\n",
    "                df_edge_embeddings_concat_filter[\"labels\"] = clf.labels_\n",
    "\n",
    "                # save\n",
    "                df_edge_embeddings_concat_filter.to_csv(\"output/{}/edge_embeddings/edge-embeddings_concat_outlier_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration), index=False)\n",
    "                \n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter[df_edge_embeddings_concat_filter[\"labels\"] == 0].copy()\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter.iloc[:, :-1]\n",
    "\n",
    "                # dict_df_edge_embeddings_concat_outlier[group] = X_train\n",
    "                dict_df_edge_embeddings_concat_filter[group] = df_edge_embeddings_concat_filter\n",
    "                \n",
    "            # mapping idx with id\n",
    "            for group in tqdm(groups_id):\n",
    "                df_aux = pd.DataFrame(())\n",
    "                k = 0\n",
    "                for subgroup in subgroups_id_op[group]:\n",
    "                    df_nodes = pd.read_csv(\"output/{}/preprocessing/graphs_data/nodes_data_{}_{}.csv\".format(exp, group, subgroup))\n",
    "                    dict_id = dict(zip(df_nodes[\"idx\"], df_nodes[\"id\"]))\n",
    "\n",
    "                    # mapping\n",
    "                    df_edge_embeddings_concat_filter = dict_df_edge_embeddings_concat_filter[group]\n",
    "                    df_edge_embeddings_concat_filter_aux = df_edge_embeddings_concat_filter[df_edge_embeddings_concat_filter[\"subgroup\"] == k].copy()\n",
    "                    \n",
    "                    # print(df_edge_embeddings_concat_filter)\n",
    "                    df_edge_embeddings_concat_filter_aux[\"source\"] = df_edge_embeddings_concat_filter_aux[\"source\"].map(dict_id)\n",
    "                    df_edge_embeddings_concat_filter_aux[\"target\"] = df_edge_embeddings_concat_filter_aux[\"target\"].map(dict_id)\n",
    "                    df_aux = pd.concat([df_aux, df_edge_embeddings_concat_filter_aux])\n",
    "                    k += 1\n",
    "                dict_df_edge_embeddings_concat_filter[group] = df_aux\n",
    "                \n",
    "            # format id\n",
    "            if data_variation != \"none\":\n",
    "                for group in tqdm(groups_id):\n",
    "                    # format\n",
    "                    df_edge_embeddings_concat_filter = dict_df_edge_embeddings_concat_filter[group]\n",
    "                    df_edge_embeddings_concat_filter[\"source\"] = df_edge_embeddings_concat_filter[\"source\"].map(lambda x: int(x[1:]))\n",
    "                    df_edge_embeddings_concat_filter[\"target\"] = df_edge_embeddings_concat_filter[\"target\"].map(lambda x: int(x[1:]))\n",
    "                        \n",
    "            # filter by different edges\n",
    "            if data_variation != \"none\":\n",
    "                for group in tqdm(groups_id):\n",
    "                    df_edge_embeddings_concat_filter = dict_df_edge_embeddings_concat_filter[group]\n",
    "                    df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter[df_edge_embeddings_concat_filter[\"source\"] != df_edge_embeddings_concat_filter[\"target\"]].copy()\n",
    "                    dict_df_edge_embeddings_concat_filter[group] = df_edge_embeddings_concat_filter\n",
    "                    \n",
    "            # count edges and filter by count\n",
    "            dict_df_edges_filter = {}\n",
    "            for group in tqdm(groups_id):\n",
    "                # read\n",
    "                df_edge_embeddings_concat_filter = dict_df_edge_embeddings_concat_filter[group]\n",
    "                \n",
    "                # sort edges\n",
    "                sort_df_edges(df_edge_embeddings_concat_filter)\n",
    "\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter[[\"source\", \"target\"]].value_counts().to_frame()\n",
    "                df_edge_embeddings_concat_filter.reset_index(inplace=True)\n",
    "                df_edge_embeddings_concat_filter.columns = [\"source\", \"target\", \"count\"]\n",
    "                \n",
    "                # filter\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter[df_edge_embeddings_concat_filter[\"count\"] == len(subgroups_id[group])]\n",
    "                df_edge_embeddings_concat_filter = df_edge_embeddings_concat_filter.iloc[:, [0, 1]]\n",
    "                dict_df_edges_filter[group] = df_edge_embeddings_concat_filter\n",
    "                \n",
    "                df_edge_embeddings_concat_filter.sort_values([\"source\", \"target\"], ascending=True, inplace=True)\n",
    "                df_edge_embeddings_concat_filter.to_csv(\"output/{}/common_edges/common_edges_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 809\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17470\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13506\n",
      "dgi-tran Nueva none\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "dgi-tran Nueva str\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "dgi-tran Nueva dyn\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "dgi-tran Vieja none\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "dgi-tran Vieja str\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "dgi-tran Vieja dyn\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "argva-base Nueva none\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "argva-base Nueva str\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "argva-base Nueva dyn\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "argva-base Vieja none\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "argva-base Vieja str\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "argva-base Vieja dyn\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "vgae-line Nueva none\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "vgae-line Nueva str\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "vgae-line Nueva dyn\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "vgae-line Vieja none\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "vgae-line Vieja str\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "vgae-line Vieja dyn\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "vgae-base Nueva none\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "vgae-base Nueva str\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 42\n",
      "Count zero:\t 29\n",
      "Count positive:\t 814\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 17666\n",
      "Count zero:\t 0\n",
      "Count positive:\t 13663\n",
      "vgae-base Nueva dyn\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "vgae-base Vieja none\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "vgae-base Vieja str\n",
      "Union\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 60\n",
      "Count zero:\t 41\n",
      "Count positive:\t 779\n",
      "Checking dataset\n",
      "Count infinite:\t 0\n",
      "Count nan:\t 0\n",
      "Count negative:\t 19050\n",
      "Count zero:\t 0\n",
      "Count positive:\t 11926\n",
      "vgae-base Vieja dyn\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "list_details = []\n",
    "\n",
    "for method in methods:\n",
    "    for k, group in enumerate(groups_id): #\n",
    "        dict_df_edges_filter = {}\n",
    "        dict_df_corr = {}\n",
    "        dict_df_edges_filter_weight = {}\n",
    "    \n",
    "        for data_variation in data_variations:\n",
    "            list_common_subgraph = []\n",
    "            for iteration in range(iterations):\n",
    "                df_edges_filter_weight_filter = pd.read_csv(\"output/{}/common_edges/common_edges_{}_{}_{}_{}.csv\".format(exp, method, group, data_variation, iteration))\n",
    "                # print(df_edges_filter_weight_filter)\n",
    "\n",
    "                G = nx.from_pandas_edgelist(df_edges_filter_weight_filter) # last change: create_using=nx.Graph() # , edge_attr=[\"weight\"])\n",
    "                # SG = G.subgraph([0, 1, 2, 3, 4, 5])\n",
    "                # graph_partial_detail(SG, edges=True)\n",
    "                list_common_subgraph.append(G)\n",
    "                \n",
    "            print(\"Union\")\n",
    "            # union\n",
    "            U = nx.compose_all(list_common_subgraph)\n",
    "            \n",
    "            df_compose_subgraph = nx.to_pandas_edgelist(U)\n",
    "            dict_df_edges_filter[group] = df_compose_subgraph.iloc[:, [0, 1]]\n",
    "            \n",
    "            # new correlation\n",
    "            nodes = list(U.nodes())\n",
    "            # print(len(nodes)) #\n",
    "            \n",
    "            df_join_raw_filter = df_join_raw_log.loc[nodes, :]\n",
    "            # check_dataset(df_join_raw_filter) #\n",
    "            # print(df_join_raw_filter.describe()) #\n",
    "            df_join_raw_filter = df_join_raw_filter.filter(like=group, axis=1)\n",
    "\n",
    "            df_join_raw_filter_t= df_join_raw_filter.T\n",
    "            check_dataset(df_join_raw_filter_t)\n",
    "            # df_join_raw_filter_corr = df_join_raw_filter_t.corr(method=\"pearson\")\n",
    "            df_join_raw_filter_corr = pg.pcorr(df_join_raw_filter_t)\n",
    "            check_dataset(df_join_raw_filter_corr)\n",
    "            dict_df_corr[group] = df_join_raw_filter_corr\n",
    "            \n",
    "            # get new correlation\n",
    "            df_edges_filter_weight = dict_df_edges_filter[group].copy()\n",
    "            df_corr = dict_df_corr[group]\n",
    "\n",
    "            df_edges_filter_weight[\"weight\"] = df_edges_filter_weight.apply(lambda x: df_corr.loc[x[\"source\"], x[\"target\"]], axis=1)\n",
    "            df_edges_filter_weight.sort_values([\"source\", \"target\"], ascending=True, inplace=True)\n",
    "            dict_df_edges_filter_weight[group] = df_edges_filter_weight\n",
    "            \n",
    "            # common subgraph\n",
    "            df_edges_filter_weight = dict_df_edges_filter_weight[group]\n",
    "            # G = nx.from_pandas_edgelist(df_edges_filter_weight, \"source\", \"target\", edge_attr=\"weight\")\n",
    "            print(method, group, data_variation)\n",
    "            # print(\"Before\")\n",
    "            # graph_partial_detail(G, edges=True)\n",
    "                \n",
    "            # filter by abs(weight) >= threshold\n",
    "            df_edges_filter_weight = dict_df_edges_filter_weight[group]\n",
    "            df_edges_filter_weight_filter = df_edges_filter_weight[df_edges_filter_weight[\"weight\"].abs() >= threshold_corr]\n",
    "            df_edges_filter_weight_filter.to_csv(\"output/{}/common_edges/common_edges_{}_{}_{}.csv\".format(exp, method, group, data_variation), index=False)\n",
    "            \n",
    "            # print(\"After\")\n",
    "            # graph_partial_detail(G, edges=True)\n",
    "            G = nx.from_pandas_edgelist(df_edges_filter_weight_filter, \"source\", \"target\", edge_attr=\"weight\")\n",
    "            list_details.append([method, group, data_variation, G.number_of_nodes(), G.number_of_edges(), nx.density(G)])\n",
    "\n",
    "df_details = pd.DataFrame(list_details, columns=[\"Method\", \"Group\", \"Data var.\", \"Num. nodes\", \"Num. edges\", \"Density\"])\n",
    "df_details.to_csv(\"output/{}/common_edges/summary.csv\".format(exp), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
